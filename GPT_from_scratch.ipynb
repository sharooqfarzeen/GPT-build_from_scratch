{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/VoUEwucYIKl6O860yFCH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Building a GPT from scratch"],"metadata":{"id":"VGi5ChXqlkTl"}},{"cell_type":"code","source":["import torch\n","import matplotlib.pyplot as plt"],"metadata":{"id":"LYbh3IiunELq","executionInfo":{"status":"ok","timestamp":1720595893009,"user_tz":-330,"elapsed":801,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"6H34Ehbr-Vd0","executionInfo":{"status":"ok","timestamp":1720595893009,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/sharooqfarzeen/GPT-build_from_scratch/main/input.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtEtKopH4YoR","executionInfo":{"status":"ok","timestamp":1720595893010,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"922b904e-8746-4a71-9d55-08637a54b116"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-07-10 07:18:15--  https://raw.githubusercontent.com/sharooqfarzeen/GPT-build_from_scratch/main/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt.2’\n","\n","\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n","\n","2024-07-10 07:18:15 (45.8 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n","\n"]}]},{"cell_type":"code","source":["#Loading the Shakespear dataset form input.txt\n","with open('input.txt', 'r', encoding = 'utf-8') as f:\n","  text = f.read()"],"metadata":{"id":"Gi0JRer9loqa","executionInfo":{"status":"ok","timestamp":1720595893010,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["#first 1000 characters\n","print(text[:1000])"],"metadata":{"id":"xI6R-VM7l7ws","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720595893010,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"f70e6816-7791-45af-91e4-5d0d71c8bda5"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","All:\n","We know't, we know't.\n","\n","First Citizen:\n","Let us kill him, and we'll have corn at our own price.\n","Is't a verdict?\n","\n","All:\n","No more talking on't; let it be done: away, away!\n","\n","Second Citizen:\n","One word, good citizens.\n","\n","First Citizen:\n","We are accounted poor citizens, the patricians good.\n","What authority surfeits on would relieve us: if they\n","would yield us but the superfluity, while it were\n","wholesome, we might guess they relieved us humanely;\n","but they think we are too dear: the leanness that\n","afflicts us, the object of our misery, is as an\n","inventory to particularise their abundance; our\n","sufferance is a gain to them Let us revenge this with\n","our pikes, ere we become rakes: for the gods know I\n","speak this in hunger for bread, not in thirst for revenge.\n","\n","\n"]}]},{"cell_type":"code","source":["#total number of characters\n","print(\"Total number of characters:\" ,len(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xz7JNGO_nqv3","executionInfo":{"status":"ok","timestamp":1720595893798,"user_tz":-330,"elapsed":796,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"e3aa428e-85f2-436b-defc-e17b535d5d67"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of characters: 1115394\n"]}]},{"cell_type":"code","source":["#unique characters in the dataset\n","chars = sorted(list(set(text)))\n","#vocabulary size\n","vocab_size = len(chars)\n","print(\"Character set:\", *chars, sep = \"\", end = \"\\n\\n\")\n","print(\"Vocabulary size:\", vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwRLpObUoAdL","executionInfo":{"status":"ok","timestamp":1720595893798,"user_tz":-330,"elapsed":36,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"7d16e6cc-d940-482b-d005-2590a9c9ec6e"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Character set:\n"," !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","\n","Vocabulary size: 65\n"]}]},{"cell_type":"markdown","source":["## Building Tokenizer"],"metadata":{"id":"-OIoyUgVpY0Q"}},{"cell_type":"code","source":["##creating character map\n","char_map = list(enumerate(chars))\n","char_map[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSmXS2WbpbAn","executionInfo":{"status":"ok","timestamp":1720595893799,"user_tz":-330,"elapsed":36,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"aba418d6-fd5e-4a91-e643-74e51a91ed77"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, '\\n'),\n"," (1, ' '),\n"," (2, '!'),\n"," (3, '$'),\n"," (4, '&'),\n"," (5, \"'\"),\n"," (6, ','),\n"," (7, '-'),\n"," (8, '.'),\n"," (9, '3')]"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["#encoding map\n","encode_map = {ch:i for i,ch in char_map}\n","#decoding map\n","decode_map = {i:ch for i,ch in char_map}\n","print(encode_map)\n","print(decode_map)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yW4JFrxQqjkC","executionInfo":{"status":"ok","timestamp":1720595893799,"user_tz":-330,"elapsed":32,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"e1421959-8d43-4c5f-f4f7-74646a833f2a"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n","{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"]}]},{"cell_type":"markdown","source":["### Encoder"],"metadata":{"id":"xvjs9v4xZFGd"}},{"cell_type":"code","source":["def encoder(text: str):\n","  encoded_arr = [encode_map[s] for s in text]\n","  return encoded_arr"],"metadata":{"id":"ntVK5ybSrbD4","executionInfo":{"status":"ok","timestamp":1720595893799,"user_tz":-330,"elapsed":29,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}}},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":["### Decoder"],"metadata":{"id":"lHI29jc0ZuBs"}},{"cell_type":"code","source":["def decoder(arr):\n","  decoded_text = ''.join([decode_map[i] for i in arr])\n","  return decoded_text"],"metadata":{"id":"Zs3GuZXPsCae","executionInfo":{"status":"ok","timestamp":1720595893800,"user_tz":-330,"elapsed":30,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["#testing encoder and decoder\n","arr = encoder(\"ABCD\")\n","arr"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ryGHLm3TskVB","executionInfo":{"status":"ok","timestamp":1720595893800,"user_tz":-330,"elapsed":30,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"1cbab98d-d34a-44d8-b4b7-b7a8cab9998e"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[13, 14, 15, 16]"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["decoder(arr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"sx5vl61dst8U","executionInfo":{"status":"ok","timestamp":1720595893800,"user_tz":-330,"elapsed":27,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"948dad8e-186a-4816-e384-a45d647d2f38"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ABCD'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":56}]},{"cell_type":"markdown","source":["## Tokenizing Shakespear dataset"],"metadata":{"id":"CSvUQV6etKyd"}},{"cell_type":"code","source":["encoded_shakespear = encoder(text)\n","encoded_shakespear[:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZTzg2u1ftPhh","executionInfo":{"status":"ok","timestamp":1720595893801,"user_tz":-330,"elapsed":26,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"0666a9dd-f1c7-47e6-f302-a233d0b5878c"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["#converting to tensor for parallel processing\n","import torch\n","data = torch.tensor(encoded_shakespear, dtype=torch.long, device=device)\n","print(data.shape, data.dtype)\n","print(data[:20])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JNNeEi_jtXOG","executionInfo":{"status":"ok","timestamp":1720595893801,"user_tz":-330,"elapsed":24,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"fe92f5f9-ab84-487f-b369-53cf2aab62cc"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1115394]) torch.int64\n","tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n","        53, 56])\n"]}]},{"cell_type":"markdown","source":["## Splitting data into training and validation sets"],"metadata":{"id":"cxYWAOw6vnba"}},{"cell_type":"code","source":["n = int(0.9*len(data))\n","train_data = data[:n] #first 90% will be train data\n","val_data = data[n:] #last 10% will be test data"],"metadata":{"id":"iuQ2kVIevrmY","executionInfo":{"status":"ok","timestamp":1720595893801,"user_tz":-330,"elapsed":22,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}}},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":["## Manually creating batches"],"metadata":{"id":"Gy5CYZlaA5Mo"}},{"cell_type":"code","source":["#setting batch size = 4 and block size = 8\n","batch_size = 4 #number of blocks to process parallely\n","block_size = 8 #size of each block/context length"],"metadata":{"id":"LlimjQ9IA9Gm","executionInfo":{"status":"ok","timestamp":1720595893801,"user_tz":-330,"elapsed":22,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","\n","def get_batch(split):\n","  data = train_data if split == \"train\" else val_data\n","  #generating 4 random numbers between between 0 and total data length - block_size\n","  ix = torch.randint(low=0, high=len(data)-block_size, size=(batch_size,), device=device)\n","  x = torch.stack([data[i: i+block_size] for i in ix])\n","  y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n","  return x, y\n","\n","xb, yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print('----')\n","\n","for b in range(batch_size): # batch dimension\n","    for t in range(block_size): # time dimension\n","        context = xb[b, :t+1]\n","        target = yb[b,t]\n","        print(f\"when input is {context.tolist()} the target: {target}\")"],"metadata":{"id":"XuZulCy_B8Xq","executionInfo":{"status":"ok","timestamp":1720595893801,"user_tz":-330,"elapsed":21,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2f700173-80f4-4549-8ed0-6828a3cad169"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs:\n","torch.Size([4, 8])\n","tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n","        [44, 53, 56,  1, 58, 46, 39, 58],\n","        [52, 58,  1, 58, 46, 39, 58,  1],\n","        [25, 17, 27, 10,  0, 21,  1, 54]])\n","targets:\n","torch.Size([4, 8])\n","tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n","        [53, 56,  1, 58, 46, 39, 58,  1],\n","        [58,  1, 58, 46, 39, 58,  1, 46],\n","        [17, 27, 10,  0, 21,  1, 54, 39]])\n","----\n","when input is [24] the target: 43\n","when input is [24, 43] the target: 58\n","when input is [24, 43, 58] the target: 5\n","when input is [24, 43, 58, 5] the target: 57\n","when input is [24, 43, 58, 5, 57] the target: 1\n","when input is [24, 43, 58, 5, 57, 1] the target: 46\n","when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n","when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n","when input is [44] the target: 53\n","when input is [44, 53] the target: 56\n","when input is [44, 53, 56] the target: 1\n","when input is [44, 53, 56, 1] the target: 58\n","when input is [44, 53, 56, 1, 58] the target: 46\n","when input is [44, 53, 56, 1, 58, 46] the target: 39\n","when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n","when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n","when input is [52] the target: 58\n","when input is [52, 58] the target: 1\n","when input is [52, 58, 1] the target: 58\n","when input is [52, 58, 1, 58] the target: 46\n","when input is [52, 58, 1, 58, 46] the target: 39\n","when input is [52, 58, 1, 58, 46, 39] the target: 58\n","when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n","when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n","when input is [25] the target: 17\n","when input is [25, 17] the target: 27\n","when input is [25, 17, 27] the target: 10\n","when input is [25, 17, 27, 10] the target: 0\n","when input is [25, 17, 27, 10, 0] the target: 21\n","when input is [25, 17, 27, 10, 0, 21] the target: 1\n","when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n","when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"]}]},{"cell_type":"code","source":["print(xb) # our input to the transformer"],"metadata":{"id":"pA1hdXptHp7b","executionInfo":{"status":"ok","timestamp":1720595893801,"user_tz":-330,"elapsed":19,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"54411226-3eb2-4008-e4a4-b63cf690e6e8"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n","        [44, 53, 56,  1, 58, 46, 39, 58],\n","        [52, 58,  1, 58, 46, 39, 58,  1],\n","        [25, 17, 27, 10,  0, 21,  1, 54]])\n"]}]},{"cell_type":"markdown","source":["## Defining the Model"],"metadata":{"id":"fqHjlZFLFIl6"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","torch.manual_seed(1337)\n","\n","class BigramLanguageModel(nn.Module):\n","\n","  def __init__(self, vocab_size):\n","    super().__init__()\n","    #embedding table\n","    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size, device=device)\n","\n","  def forward(self, idx, targets=None): #targets=None when generating\n","\n","    #idx and targets are both tensors of shape B*T\n","    #where B is batch size and T is block size\n","    logits = self.token_embedding_table(idx) #B*T*C shaped tensor, where C is channel/vocab_size(65)\n","    \"\"\"\n","    idx contains a 4*8 tensor where each element is the token number of a character\n","    token_embedding_table(idx) plucks out the row corresponding to a particular token number from the 65*65 embedding table,\n","    and returns a 4*8*65 tensor\n","    \"\"\"\n","    if targets is not None: #training run => loss has to be calculated\n","      #F.cross_entropy requires input(logits) to be of shape (B*T),(C), whereas our logits is of shape B*T*C\n","      B, T, C = logits.shape\n","      logits = logits.view(B*T, C)\n","      targets = targets.view(B*T)\n","      #measuring loss\n","      loss = F.cross_entropy(logits, targets)\n","    else:\n","      loss = None\n","\n","    return logits, loss\n","\n","  def generate(self, idx, max_new_tokens):\n","    #idx is a (B,T) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","      #generate predictions\n","      logits, loss = self(idx) #calls forward function\n","      #filter last column out, to focus only on the last time step\n","      logits = logits[:, -1, :] #becomes (B,C)\n","      #apply softmax to get probabilities\n","      probs = F.softmax(logits, dim=-1) #(B,C)\n","      #sample from distribution\n","      idx_next = torch.multinomial(probs, num_samples=1) #(B, 1)\n","      #append sampled index to running sequence\n","      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","\n","    return idx\n","\n","m = BigramLanguageModel(vocab_size)\n","out, loss = m(xb, yb)\n","print(out.shape, loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mpSDFmlzFKif","executionInfo":{"status":"ok","timestamp":1720595893802,"user_tz":-330,"elapsed":17,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"275df03d-3f62-47a5-9778-1a0cf93c41a6"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 65]) tensor(4.8786, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"markdown","source":["## Text Generator"],"metadata":{"id":"zXxS40CoZ4bg"}},{"cell_type":"code","source":["def make_more(idx):\n","  generated_indices = m.generate(idx, max_new_tokens=100)\n","  #predictions\n","  predicted_indices = generated_indices[0]\n","  #converting to python list\n","  index_list = generated_indices.tolist()[0]\n","  #decoding\n","  output = decoder(index_list)\n","  return output"],"metadata":{"id":"lv5swgRhZ_ex","executionInfo":{"status":"ok","timestamp":1720595893802,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["#sample output before training\n","idx = torch.zeros((1,1), dtype=torch.long, device=device) #(1,1) tensor with index 0; index 0 corresponds to the new line character\n","print(make_more(idx))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4cxUOSFyfJFQ","executionInfo":{"status":"ok","timestamp":1720595893802,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"ec352cf7-61f9-4cf1-861e-eede7c63f9bd"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"]}]},{"cell_type":"markdown","source":["Test generated before training the neural net for prompt \"\\n\":\n","```\n","Fw:IQPpA.k,fDs'a3JxzIjo;:\n","Yow&$FmtofC,?-LB!HgV!?W;KuibN,,ge3 ibaeYEYBgc$n;lxP;HFGEdkoG EsSXZ3;qGknG.\n","```\n"],"metadata":{"id":"Z8FiML0Nc235"}},{"cell_type":"markdown","source":["## Training the Neural Net"],"metadata":{"id":"q3AgOTI_gt4z"}},{"cell_type":"code","source":["#PyTorch Optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"],"metadata":{"id":"ojD7nQOdhPhc","executionInfo":{"status":"ok","timestamp":1720596550049,"user_tz":-330,"elapsed":621,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["step_loss = [3.0998282432556152 2.7639319896698 2.6211414337158203 2.4793190956115723]"],"metadata":{"id":"F7ofbUe3i6d6","executionInfo":{"status":"ok","timestamp":1720596552198,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","\n","for steps in range(1000):\n","\n","  #sample a batch of data\n","  xb, yb = get_batch('train')\n","\n","  #evaluate the loss\n","  logits, loss = m(xb, yb)\n","  #set gradients to zero\n","  optimizer.zero_grad(set_to_none=True)\n","  #backpropagation\n","  loss.backward()\n","  #optimize parameters\n","  optimizer.step()\n","  if steps % 1000 == 0:\n","    step_loss.append(loss.item())\n","\n","print(*step_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hJdpj7ZbgzNs","executionInfo":{"status":"ok","timestamp":1720596599502,"user_tz":-330,"elapsed":2560,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"53ec06c6-f216-4541-af90-7858d4e19b98"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["3.0998282432556152 2.7639319896698 2.6211414337158203 2.4793190956115723\n"]}]},{"cell_type":"markdown","source":["### Test Generation"],"metadata":{"id":"wKtnZl6ljOfh"}},{"cell_type":"code","source":["idx = torch.zeros((1,1), dtype=torch.long, device=device) #(1,1) tensor with index 0; index 0 corresponds to the new line character\n","print(make_more(idx))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzM8O2zHjP2D","executionInfo":{"status":"ok","timestamp":1720596662905,"user_tz":-330,"elapsed":623,"user":{"displayName":"Sharooq Farzeen Abdul Khayoom","userId":"00254559679343624974"}},"outputId":"b4a2e81f-3829-4d52-bf4d-e7814204c695"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","I's, ar mifand IGes cer herish KI ster \n","\n","The athan y BOnk wndeousik mon ied he y.\n","tht l acke siespre\n"]}]},{"cell_type":"markdown","source":["Test Generation with loss = 2.4793190956115723\n","\n","\n","```\n","I's, ar mifand IGes cer herish KI ster\n","\n","The athan y BOnk wndeousik mon ied he y.\n","tht l acke *siespre*\n","```\n","\n"],"metadata":{"id":"jiEAg73TjdXu"}},{"cell_type":"markdown","source":["## Complete Code(copied from https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=hoelkOrFY8bN)\n"],"metadata":{"id":"UcjGnPaknA42"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","# hyperparameters\n","batch_size = 16 # how many independent sequences will we process in parallel?\n","block_size = 32 # what is the maximum context length for predictions?\n","max_iters = 5000\n","eval_interval = 100\n","learning_rate = 1e-3\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embd = 64\n","n_head = 4\n","n_layer = 4\n","dropout = 0.0\n","# ------------\n","\n","torch.manual_seed(1337)\n","\n","# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","with open('input.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# data loading\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,C)\n","        q = self.query(x) # (B,T,C)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,C)\n","        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","# super simple bigram model\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","        x = tok_emb + pos_emb # (B,T,C)\n","        x = self.blocks(x) # (B,T,C)\n","        x = self.ln_f(x) # (B,T,C)\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n","\n","model = BigramLanguageModel()\n","m = model.to(device)\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","# generate from the model\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"],"metadata":{"id":"jQHoBLGqjgq-"},"execution_count":null,"outputs":[]}]}